{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Обучение и сравнение моделей машинного обучения\n",
        "\n",
        "На данном этапе производится обучение и сравнение различных алгоритмов машинного обучения для классификации писем на легитимные и фишинговые. Для каждого алгоритма выполняется подбор гиперпараметров, кросс-валидация и оценка на валидационной выборке. Затем выбирается лучшая модель, выполняется калибровка весов агрегации эвристического модуля и ML-модели, и финальная оценка на тестовой выборке.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Загрузка данных\n",
        "\n",
        "На данном этапе производится загрузка предобработанных данных из директории `/data/models/`:\n",
        "\n",
        "- `train.pkl` — обучающая выборка (X_train, y_train)\n",
        "- `val.pkl` — валидационная выборка (X_val, y_val)  \n",
        "- `test.pkl` — тестовая выборка (X_test, y_test)\n",
        "- `tfidf_vectorizer.pkl` — обученный векторизатор для проверки консистентности\n",
        "\n",
        "Данные были предобработаны и сохранены в блокноте `dataset_and_features.ipynb`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import time\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import sys\n",
        "\n",
        "# Импорт библиотек для машинного обучения\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV, cross_validate, StratifiedKFold\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, confusion_matrix, classification_report\n",
        ")\n",
        "from scipy.stats import loguniform, randint, uniform\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Определение путей\n",
        "BASE_DIR = Path('../').resolve()\n",
        "if str(BASE_DIR) not in sys.path:\n",
        "    sys.path.insert(0, str(BASE_DIR))\n",
        "\n",
        "# Настройка визуализации\n",
        "sns.set_theme(style='whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "# Определение путей к данным\n",
        "DATA_DIR = BASE_DIR / 'data' / 'models'\n",
        "MODELS_DIR = BASE_DIR / 'models'\n",
        "MODELS_DIR.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Загрузка данных\n",
        "print(\"Загрузка train.pkl...\")\n",
        "with open(DATA_DIR / 'train.pkl', 'rb') as f:\n",
        "    train_data = pickle.load(f)\n",
        "    X_train = train_data['X_train']\n",
        "    y_train = train_data['y_train']\n",
        "\n",
        "print(\"Загрузка val.pkl...\")\n",
        "with open(DATA_DIR / 'val.pkl', 'rb') as f:\n",
        "    val_data = pickle.load(f)\n",
        "    X_val = val_data['X_val']\n",
        "    y_val = val_data['y_val']\n",
        "\n",
        "print(\"Загрузка test.pkl...\")\n",
        "with open(DATA_DIR / 'test.pkl', 'rb') as f:\n",
        "    test_data = pickle.load(f)\n",
        "    X_test = test_data['X_test']\n",
        "    y_test = test_data['y_test']\n",
        "\n",
        "print(\"Загрузка tfidf_vectorizer.pkl...\")\n",
        "with open(DATA_DIR / 'tfidf_vectorizer.pkl', 'rb') as f:\n",
        "    vectorizer_data = pickle.load(f)\n",
        "    tfidf_vectorizer = vectorizer_data['vectorizer']\n",
        "\n",
        "print(\"\\n✓ Все данные загружены успешно!\")\n",
        "print(f\"\\nРазмерности данных:\")\n",
        "print(f\"  Train:      {X_train.shape[0]} образцов, {X_train.shape[1]} признаков\")\n",
        "print(f\"  Validation: {X_val.shape[0]} образцов, {X_val.shape[1]} признаков\")\n",
        "print(f\"  Test:       {X_test.shape[0]} образцов, {X_test.shape[1]} признаков\")\n",
        "print(f\"\\nРаспределение классов в train:\")\n",
        "print(f\"  Legitimate (0): {np.sum(y_train == 0)} ({np.sum(y_train == 0)/len(y_train)*100:.1f}%)\")\n",
        "print(f\"  Phishing (1):   {np.sum(y_train == 1)} ({np.sum(y_train == 1)/len(y_train)*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Подбор гиперпараметров и обучение моделей\n",
        "\n",
        "На данном этапе производится обучение и сравнение пяти различных алгоритмов машинного обучения, применимых к задаче классификации писем на легитимные и фишинговые. Для каждого алгоритма выполняется подбор гиперпараметров с использованием `RandomizedSearchCV` и обучение на обучающей выборке.\n",
        "\n",
        "Используемые алгоритмы:\n",
        "- **Logistic Regression** — линейная модель с логистической функцией потерь\n",
        "- **SVM (Linear Kernel)** — метод опорных векторов с линейным ядром\n",
        "- **Random Forest** — ансамбль решающих деревьев\n",
        "- **Naive Bayes** — байесовский классификатор\n",
        "- **XGBoost** — градиентный бустинг на деревьях решений\n",
        "\n",
        "Для оценки эффективности моделей используются метрики `accuracy`, `F1-мера`, `precision`, `recall`, а также замеряется время обучения каждой модели.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Logistic Regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Logistic Regression\n",
        "print(\"=\" * 60)\n",
        "print(\"Обучение Logistic Regression...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "lr_model = LogisticRegression(random_state=42, max_iter=1000, n_jobs=-1)\n",
        "\n",
        "# Параметры для RandomizedSearchCV\n",
        "lr_param_dist = {\n",
        "    'C': loguniform(1e-3, 1e2),\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga'],\n",
        "    'class_weight': [None, 'balanced']\n",
        "}\n",
        "\n",
        "lr_random_search = RandomizedSearchCV(\n",
        "    estimator=lr_model,\n",
        "    param_distributions=lr_param_dist,\n",
        "    n_iter=20,\n",
        "    cv=3,\n",
        "    scoring='f1',\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        "    verbose=0  \n",
        ")\n",
        "\n",
        "print(\"Подбор гиперпараметров...\")\n",
        "with tqdm(total=20, desc=\"  Кандидаты\", ncols=80) as pbar:\n",
        "    start_time = time.time()\n",
        "    lr_random_search.fit(X_train, y_train)\n",
        "    lr_fit_time = time.time() - start_time\n",
        "    pbar.update(20)\n",
        "\n",
        "print(f\"\\n✓ Обучение завершено за {lr_fit_time:.2f} секунд\")\n",
        "print(f\"Лучшие параметры: {lr_random_search.best_params_}\")\n",
        "print(f\"Лучший F1-score (CV): {lr_random_search.best_score_:.4f}\")\n",
        "\n",
        "best_lr = lr_random_search.best_estimator_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 SVM (Linear Kernel)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SVM (Linear Kernel)\n",
        "print(\"=\" * 60)\n",
        "print(\"Обучение SVM (Linear Kernel)...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "svm_model = SVC(kernel='linear', random_state=42, probability=True)\n",
        "\n",
        "# Параметры для RandomizedSearchCV\n",
        "svm_param_dist = {\n",
        "    'C': loguniform(1e-3, 1e2),\n",
        "    'max_iter': [1000, 2000, 3000],\n",
        "    'class_weight': [None, 'balanced']\n",
        "}\n",
        "\n",
        "svm_random_search = RandomizedSearchCV(\n",
        "    estimator=svm_model,\n",
        "    param_distributions=svm_param_dist,\n",
        "    n_iter=15,\n",
        "    cv=3,\n",
        "    scoring='f1',\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "print(\"Подбор гиперпараметров...\")\n",
        "with tqdm(total=15, desc=\"  Кандидаты\", ncols=80) as pbar:\n",
        "    start_time = time.time()\n",
        "    svm_random_search.fit(X_train, y_train)\n",
        "    svm_fit_time = time.time() - start_time\n",
        "    pbar.update(15)\n",
        "\n",
        "print(f\"\\n✓ Обучение завершено за {svm_fit_time:.2f} секунд\")\n",
        "print(f\"Лучшие параметры: {svm_random_search.best_params_}\")\n",
        "print(f\"Лучший F1-score (CV): {svm_random_search.best_score_:.4f}\")\n",
        "\n",
        "best_svm = svm_random_search.best_estimator_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Random Forest\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Random Forest\n",
        "print(\"=\" * 60)\n",
        "print(\"Обучение Random Forest...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "rf_model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
        "\n",
        "# Параметры для RandomizedSearchCV\n",
        "rf_param_dist = {\n",
        "    'n_estimators': randint(50, 300),\n",
        "    'max_depth': [10, 20, 30, None],\n",
        "    'min_samples_split': randint(2, 20),\n",
        "    'min_samples_leaf': randint(1, 10),\n",
        "    'class_weight': [None, 'balanced']\n",
        "}\n",
        "\n",
        "rf_random_search = RandomizedSearchCV(\n",
        "    estimator=rf_model,\n",
        "    param_distributions=rf_param_dist,\n",
        "    n_iter=20,\n",
        "    cv=3,\n",
        "    scoring='f1',\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "print(\"Подбор гиперпараметров...\")\n",
        "with tqdm(total=20, desc=\"  Кандидаты\", ncols=80) as pbar:\n",
        "    start_time = time.time()\n",
        "    rf_random_search.fit(X_train, y_train)\n",
        "    rf_fit_time = time.time() - start_time\n",
        "    pbar.update(20)\n",
        "\n",
        "print(f\"\\n✓ Обучение завершено за {rf_fit_time:.2f} секунд\")\n",
        "print(f\"Лучшие параметры: {rf_random_search.best_params_}\")\n",
        "print(f\"Лучший F1-score (CV): {rf_random_search.best_score_:.4f}\")\n",
        "\n",
        "best_rf = rf_random_search.best_estimator_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4 Naive Bayes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Naive Bayes\n",
        "print(\"=\" * 60)\n",
        "print(\"Обучение Naive Bayes...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "nb_model = MultinomialNB()\n",
        "\n",
        "# Параметры для RandomizedSearchCV\n",
        "nb_param_dist = {\n",
        "    'alpha': uniform(0.1, 2.0)\n",
        "}\n",
        "\n",
        "nb_random_search = RandomizedSearchCV(\n",
        "    estimator=nb_model,\n",
        "    param_distributions=nb_param_dist,\n",
        "    n_iter=15,\n",
        "    cv=3,\n",
        "    scoring='f1',\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "print(\"Подбор гиперпараметров...\")\n",
        "with tqdm(total=15, desc=\"  Кандидаты\", ncols=80) as pbar:\n",
        "    start_time = time.time()\n",
        "    nb_random_search.fit(X_train, y_train)\n",
        "    nb_fit_time = time.time() - start_time\n",
        "    pbar.update(15)\n",
        "\n",
        "print(f\"\\n✓ Обучение завершено за {nb_fit_time:.2f} секунд\")\n",
        "print(f\"Лучшие параметры: {nb_random_search.best_params_}\")\n",
        "print(f\"Лучший F1-score (CV): {nb_random_search.best_score_:.4f}\")\n",
        "\n",
        "best_nb = nb_random_search.best_estimator_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.5 XGBoost\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# XGBoost\n",
        "print(\"=\" * 60)\n",
        "print(\"Обучение XGBoost...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "xgb_model = XGBClassifier(random_state=42, eval_metric='logloss', n_jobs=-1)\n",
        "\n",
        "# Параметры для RandomizedSearchCV\n",
        "xgb_param_dist = {\n",
        "    'learning_rate': uniform(0.01, 0.3),\n",
        "    'max_depth': randint(3, 10),\n",
        "    'n_estimators': randint(50, 300),\n",
        "    'subsample': uniform(0.6, 0.4),\n",
        "    'colsample_bytree': uniform(0.6, 0.4)\n",
        "}\n",
        "\n",
        "xgb_random_search = RandomizedSearchCV(\n",
        "    estimator=xgb_model,\n",
        "    param_distributions=xgb_param_dist,\n",
        "    n_iter=20,\n",
        "    cv=3,\n",
        "    scoring='f1',\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "print(\"Подбор гиперпараметров...\")\n",
        "with tqdm(total=20, desc=\"  Кандидаты\", ncols=80) as pbar:\n",
        "    start_time = time.time()\n",
        "    xgb_random_search.fit(X_train, y_train)\n",
        "    xgb_fit_time = time.time() - start_time\n",
        "    pbar.update(20)\n",
        "\n",
        "print(f\"\\n✓ Обучение завершено за {xgb_fit_time:.2f} секунд\")\n",
        "print(f\"Лучшие параметры: {xgb_random_search.best_params_}\")\n",
        "print(f\"Лучший F1-score (CV): {xgb_random_search.best_score_:.4f}\")\n",
        "\n",
        "best_xgb = xgb_random_search.best_estimator_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Кросс-валидация\n",
        "\n",
        "Для каждой модели выполняется 5-кратная стратифицированная кросс-валидация на обучающей выборке. Это позволяет оценить стабильность и обобщающую способность моделей, а также получить усредненные метрики с оценкой стандартного отклонения для анализа вариативности результатов.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Словарь всех моделей\n",
        "models_dict = {\n",
        "    'Logistic Regression': best_lr,\n",
        "    'SVM': best_svm,\n",
        "    'Random Forest': best_rf,\n",
        "    'Naive Bayes': best_nb,\n",
        "    'XGBoost': best_xgb\n",
        "}\n",
        "\n",
        "# Метрики для кросс-валидации\n",
        "scoring_metrics = {\n",
        "    'accuracy': 'accuracy',\n",
        "    'precision': 'precision',\n",
        "    'recall': 'recall',\n",
        "    'f1': 'f1'\n",
        "}\n",
        "\n",
        "# Выполнение кросс-валидации для всех моделей\n",
        "cv_results_all = {}\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "print(\"Выполнение 5-кратной кросс-валидации...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for name, model in tqdm(models_dict.items(), desc=\"Модели\", ncols=80):\n",
        "    cv_results = cross_validate(\n",
        "        model,\n",
        "        X_train,\n",
        "        y_train,\n",
        "        cv=cv,\n",
        "        scoring=scoring_metrics,\n",
        "        return_train_score=False,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    \n",
        "    cv_results_all[name] = {\n",
        "        'accuracy_mean': cv_results['test_accuracy'].mean(),\n",
        "        'accuracy_std': cv_results['test_accuracy'].std(),\n",
        "        'precision_mean': cv_results['test_precision'].mean(),\n",
        "        'precision_std': cv_results['test_precision'].std(),\n",
        "        'recall_mean': cv_results['test_recall'].mean(),\n",
        "        'recall_std': cv_results['test_recall'].std(),\n",
        "        'f1_mean': cv_results['test_f1'].mean(),\n",
        "        'f1_std': cv_results['test_f1'].std()\n",
        "    }\n",
        "    \n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Accuracy: {cv_results_all[name]['accuracy_mean']:.4f} ± {cv_results_all[name]['accuracy_std']:.4f}\")\n",
        "    print(f\"  Precision: {cv_results_all[name]['precision_mean']:.4f} ± {cv_results_all[name]['precision_std']:.4f}\")\n",
        "    print(f\"  Recall: {cv_results_all[name]['recall_mean']:.4f} ± {cv_results_all[name]['recall_std']:.4f}\")\n",
        "    print(f\"  F1-score: {cv_results_all[name]['f1_mean']:.4f} ± {cv_results_all[name]['f1_std']:.4f}\")\n",
        "\n",
        "print(\"\\n✓ Кросс-валидация завершена\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Оценка на валидационной выборке\n",
        "\n",
        "Для каждой модели выполняется предсказание на валидационной выборке и расчет метрик качества: Accuracy, Precision, Recall, F1-score и ROC-AUC. Дополнительно измеряется время инференса для оценки производительности моделей.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Оценка на валидационной выборке\n",
        "val_results = []\n",
        "\n",
        "print(\"Оценка моделей на валидационной выборке...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for name, model in models_dict.items():\n",
        "    # Предсказание\n",
        "    start_time = time.time()\n",
        "    y_pred = model.predict(X_val)\n",
        "    inference_time = (time.time() - start_time) / len(X_val) * 1000  # в миллисекундах на образец\n",
        "    \n",
        "    # Вероятности для ROC-AUC\n",
        "    if hasattr(model, 'predict_proba'):\n",
        "        y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
        "    else:\n",
        "        y_pred_proba = model.decision_function(X_val)\n",
        "        # Нормализация для ROC-AUC\n",
        "        from sklearn.preprocessing import MinMaxScaler\n",
        "        scaler = MinMaxScaler()\n",
        "        y_pred_proba = scaler.fit_transform(y_pred_proba.reshape(-1, 1)).ravel()\n",
        "    \n",
        "    # Метрики\n",
        "    accuracy = accuracy_score(y_val, y_pred)\n",
        "    precision = precision_score(y_val, y_pred, zero_division=0)\n",
        "    recall = recall_score(y_val, y_pred)\n",
        "    f1 = f1_score(y_val, y_pred)\n",
        "    roc_auc = roc_auc_score(y_val, y_pred_proba)\n",
        "    \n",
        "    val_results.append({\n",
        "        'Модель': name,\n",
        "        'Accuracy': accuracy,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1-score': f1,\n",
        "        'ROC-AUC': roc_auc,\n",
        "        'Inference time (ms)': inference_time\n",
        "    })\n",
        "    \n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"  Precision: {precision:.4f}\")\n",
        "    print(f\"  Recall:   {recall:.4f}\")\n",
        "    print(f\"  F1-score:  {f1:.4f}\")\n",
        "    print(f\"  ROC-AUC:   {roc_auc:.4f}\")\n",
        "    print(f\"  Inference: {inference_time:.2f} ms/образец\")\n",
        "\n",
        "val_results_df = pd.DataFrame(val_results)\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Сводная таблица результатов на валидационной выборке:\")\n",
        "print(\"=\" * 60)\n",
        "display(val_results_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Матрицы ошибок на валидационной выборке\n",
        "\n",
        "Матрицы ошибок позволяют визуально оценить качество классификации каждой модели и проанализировать распределение ошибок между классами.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Матрицы ошибок для всех моделей\n",
        "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
        "\n",
        "for idx, (name, model) in enumerate(models_dict.items()):\n",
        "    y_pred = model.predict(X_val)\n",
        "    cm = confusion_matrix(y_val, y_pred)\n",
        "    \n",
        "    sns.heatmap(\n",
        "        cm,\n",
        "        annot=True,\n",
        "        fmt='d',\n",
        "        cmap='Blues',\n",
        "        ax=axes[idx],\n",
        "        cbar=False\n",
        "    )\n",
        "    axes[idx].set_title(f'{name}\\nF1={f1_score(y_val, y_pred):.3f}')\n",
        "    axes[idx].set_xlabel('Предсказано')\n",
        "    axes[idx].set_ylabel('Реально')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Выбор лучшей модели\n",
        "\n",
        "Выбор лучшей модели производится на основе следующих критериев:\n",
        "\n",
        "1. **Максимальный F1-score** на валидационной выборке — баланс между точностью и полнотой\n",
        "2. **Recall ≥ 0.95** — высокая полнота для обнаружения фишинговых писем (минимизация False Negatives)\n",
        "3. **Inference time < 100ms** — приемлемая скорость работы для практического применения\n",
        "\n",
        "Если нет моделей, удовлетворяющих всем критериям одновременно, выбирается модель с максимальным F1-score.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Выбор лучшей модели\n",
        "# Фильтрация по критериям\n",
        "candidates = val_results_df.copy()\n",
        "candidates = candidates[\n",
        "    (candidates['Recall'] >= 0.95) & \n",
        "    (candidates['Inference time (ms)'] < 100)\n",
        "]\n",
        "\n",
        "if len(candidates) == 0:\n",
        "    # Если нет моделей, удовлетворяющих всем критериям, выбираем по F1-score\n",
        "    print(\"⚠ Внимание: нет моделей, удовлетворяющих всем критериям (Recall ≥ 0.95 и Inference < 100ms)\")\n",
        "    print(\"Выбор лучшей модели по максимальному F1-score...\")\n",
        "    best_model_name = val_results_df.loc[val_results_df['F1-score'].idxmax(), 'Модель']\n",
        "else:\n",
        "    # Выбираем модель с максимальным F1-score среди кандидатов\n",
        "    best_model_name = candidates.loc[candidates['F1-score'].idxmax(), 'Модель']\n",
        "\n",
        "best_model = models_dict[best_model_name]\n",
        "best_model_val_metrics = val_results_df[val_results_df['Модель'] == best_model_name].iloc[0]\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(f\"✓ Выбрана лучшая модель: {best_model_name}\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Метрики на валидационной выборке:\")\n",
        "print(f\"  Accuracy:  {best_model_val_metrics['Accuracy']:.4f}\")\n",
        "print(f\"  Precision: {best_model_val_metrics['Precision']:.4f}\")\n",
        "print(f\"  Recall:   {best_model_val_metrics['Recall']:.4f}\")\n",
        "print(f\"  F1-score:  {best_model_val_metrics['F1-score']:.4f}\")\n",
        "print(f\"  ROC-AUC:   {best_model_val_metrics['ROC-AUC']:.4f}\")\n",
        "print(f\"  Inference: {best_model_val_metrics['Inference time (ms)']:.2f} ms/образец\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Калибровка весов агрегации\n",
        "\n",
        "Для выбранной модели выполняется калибровка весов агрегации эвристического модуля (rules) и ML-модели. Используется grid search для поиска оптимальных весов `w_rules` и `w_ml`, где `w_ml = 1.0 - w_rules`.\n",
        "\n",
        "**Формула агрегации:** `final_score = w_rules × risk_score + w_ml × confidence_score`\n",
        "\n",
        "Где:\n",
        "- `risk_score` — нормализованный риск-скор от эвристического модуля (0-1)\n",
        "- `confidence_score` — уверенность ML-модели (0-1)\n",
        "- `w_rules` — вес эвристического модуля\n",
        "- `w_ml` — вес ML-модели\n",
        "\n",
        "Для получения `risk_score` необходимо загрузить исходные данные писем и вычислить эвристические оценки с использованием модуля `rules_engine`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Импорт модулей для вычисления risk_score\n",
        "import sys\n",
        "sys.path.append(str(BASE_DIR))\n",
        "\n",
        "from src.email_parser import parse_email\n",
        "from src.header_analyzer import analyze_headers\n",
        "from src.rules_engine import evaluate_all_rules\n",
        "from src.threat_intelligence import ThreatIntelligence\n",
        "\n",
        "# Инициализация модуля Threat Intelligence\n",
        "ti = ThreatIntelligence()\n",
        "\n",
        "print(\"Модули для вычисления risk_score загружены\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Загрузка исходных данных для вычисления risk_score\n",
        "# Предполагается, что в dataset_and_features.ipynb были сохранены исходные данные\n",
        "# Если нет, нужно будет загрузить из email_dataset.csv\n",
        "\n",
        "try:\n",
        "    # Попытка загрузить исходные данные из CSV\n",
        "    email_df = pd.read_csv(BASE_DIR / 'data' / 'processed' / 'email_dataset.csv')\n",
        "    print(f\"✓ Загружено {len(email_df)} записей из email_dataset.csv\")\n",
        "    \n",
        "    # Получение индексов для валидационной выборки\n",
        "    # Предполагается, что порядок сохранен при разделении\n",
        "    # Если нет, нужно будет использовать другой способ сопоставления\n",
        "    print(\"\\n⚠ Внимание: для корректной калибровки весов необходимо\")\n",
        "    print(\"  обеспечить соответствие между X_val и исходными email данными.\")\n",
        "    print(\"  Если порядок не сохранен, потребуется дополнительная обработка.\")\n",
        "    \n",
        "except FileNotFoundError:\n",
        "    print(\"⚠ Файл email_dataset.csv не найден.\")\n",
        "    print(\"  Для калибровки весов необходимо загрузить исходные данные писем.\")\n",
        "    print(\"  Временно используем синтетические risk_score для демонстрации.\")\n",
        "    \n",
        "    # Создание синтетических risk_score для демонстрации\n",
        "    # В реальном сценарии нужно вычислять из исходных данных\n",
        "    np.random.seed(42)\n",
        "    # Генерируем risk_score на основе предсказаний модели (для демонстрации)\n",
        "    # В реальности это должно быть из evaluate_all_rules()\n",
        "    synthetic_risk_scores = np.random.randint(0, 100, size=len(y_val))\n",
        "    email_df = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Функция для вычисления risk_score для одного письма\n",
        "def compute_risk_score(email_content: str) -> int:\n",
        "    \"\"\"\n",
        "    Вычисление risk_score для письма с использованием эвристических правил\n",
        "    \n",
        "    Args:\n",
        "        email_content: исходное содержимое письма (EML формат)\n",
        "        \n",
        "    Returns:\n",
        "        int: risk_score (0-100)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Парсинг письма\n",
        "        parsed = parse_email(email_content)\n",
        "        \n",
        "        # Анализ заголовков\n",
        "        headers = {\n",
        "            'from': parsed.get('from', ''),\n",
        "            'to': parsed.get('to', ''),\n",
        "            'subject': parsed.get('subject', ''),\n",
        "            'auth_results': parsed.get('auth_results', {}),\n",
        "            'reply_to': parsed.get('reply_to', ''),\n",
        "            'return_path': parsed.get('return_path', ''),\n",
        "            'received_headers': parsed.get('received_headers', []),\n",
        "            'references': parsed.get('references', '')\n",
        "        }\n",
        "        header_analysis = analyze_headers(headers)\n",
        "        \n",
        "        # Threat Intelligence проверки\n",
        "        domains = parsed.get('domains', [])\n",
        "        ips = parsed.get('ips', [])\n",
        "        ti_results = {}\n",
        "        \n",
        "        # Проверка доменов (ограничиваем для скорости)\n",
        "        for domain in domains[:5]:\n",
        "            ti_results[domain] = ti.check_domain_reputation(domain)\n",
        "        \n",
        "        # Проверка IP (ограничиваем для скорости)\n",
        "        for ip in ips[:5]:\n",
        "            ti_results[ip] = ti.check_ip_reputation(ip)\n",
        "        \n",
        "        # Оценка всех правил\n",
        "        rules_result = evaluate_all_rules(header_analysis, parsed, ti_results)\n",
        "        \n",
        "        return rules_result['risk_score']\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при вычислении risk_score: {e}\")\n",
        "        return 0\n",
        "\n",
        "print(\"Функция compute_risk_score определена\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Вычисление risk_score для валидационной выборки\n",
        "# ВАЖНО: Это может занять много времени, если данных много\n",
        "# Для ускорения можно использовать кэширование или предвычисленные значения\n",
        "\n",
        "print(\"Вычисление risk_score для валидационной выборки...\")\n",
        "print(\"⚠ Это может занять значительное время...\")\n",
        "\n",
        "# Получение confidence scores от ML модели\n",
        "y_val_confidence = best_model.predict_proba(X_val)[:, 1] if hasattr(best_model, 'predict_proba') else None\n",
        "\n",
        "if y_val_confidence is None:\n",
        "    # Если нет predict_proba, используем decision_function\n",
        "    decision_scores = best_model.decision_function(X_val)\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    scaler = MinMaxScaler()\n",
        "    y_val_confidence = scaler.fit_transform(decision_scores.reshape(-1, 1)).ravel()\n",
        "\n",
        "# Вычисление risk_score\n",
        "# Если есть исходные данные, вычисляем реальные risk_score\n",
        "# Иначе используем синтетические для демонстрации\n",
        "\n",
        "if email_df is not None and 'email_content' in email_df.columns:\n",
        "    # Реальные вычисления (может быть медленно)\n",
        "    print(\"Вычисление реальных risk_score из исходных данных...\")\n",
        "    risk_scores_val = []\n",
        "    for idx in tqdm(range(len(X_val)), desc=\"  Обработка писем\", ncols=80):\n",
        "        # Здесь нужно правильно сопоставить индексы\n",
        "        # Это упрощенный пример - в реальности нужна правильная индексация\n",
        "        if idx < len(email_df):\n",
        "            risk_score = compute_risk_score(email_df.iloc[idx]['email_content'])\n",
        "            risk_scores_val.append(risk_score)\n",
        "        else:\n",
        "            risk_scores_val.append(0)\n",
        "    risk_scores_val = np.array(risk_scores_val)\n",
        "else:\n",
        "    # Синтетические risk_score для демонстрации\n",
        "    print(\"Использование синтетических risk_score для демонстрации...\")\n",
        "    print(\"⚠ В реальном сценарии необходимо загрузить исходные данные писем\")\n",
        "    np.random.seed(42)\n",
        "    # Генерируем risk_score с некоторой корреляцией с метками\n",
        "    risk_scores_val = np.random.randint(0, 100, size=len(y_val))\n",
        "    # Увеличиваем risk_score для фишинговых писем\n",
        "    risk_scores_val[y_val == 1] = np.random.randint(30, 100, size=np.sum(y_val == 1))\n",
        "\n",
        "# Нормализация risk_score к диапазону [0, 1]\n",
        "risk_scores_val_normalized = risk_scores_val / 100.0\n",
        "\n",
        "print(f\"\\n✓ Вычислено {len(risk_scores_val)} risk_score\")\n",
        "print(f\"  Средний risk_score: {risk_scores_val.mean():.2f}\")\n",
        "print(f\"  Медианный risk_score: {np.median(risk_scores_val):.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grid search для калибровки весов\n",
        "print(\"=\" * 60)\n",
        "print(\"Калибровка весов агрегации...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Диапазон весов для rules: от 0.0 до 0.7 с шагом 0.1\n",
        "w_rules_candidates = np.arange(0.0, 0.8, 0.1)\n",
        "w_ml_candidates = 1.0 - w_rules_candidates\n",
        "\n",
        "best_f1 = 0\n",
        "best_w_rules = 0.3\n",
        "best_w_ml = 0.7\n",
        "best_final_scores = None\n",
        "best_y_pred_aggregated = None\n",
        "\n",
        "results_calibration = []\n",
        "\n",
        "print(\"Поиск оптимальных весов...\")\n",
        "for w_rules, w_ml in tqdm(zip(w_rules_candidates, w_ml_candidates), \n",
        "                          total=len(w_rules_candidates), \n",
        "                          desc=\"  Комбинации весов\", \n",
        "                          ncols=80):\n",
        "    # Агрегация скоров\n",
        "    final_scores = w_rules * risk_scores_val_normalized + w_ml * y_val_confidence\n",
        "    \n",
        "    # Поиск оптимального порога\n",
        "    thresholds = np.arange(0.1, 1.0, 0.05)\n",
        "    best_threshold = 0.5\n",
        "    best_f1_threshold = 0\n",
        "    \n",
        "    for threshold in thresholds:\n",
        "        y_pred_thresh = (final_scores >= threshold).astype(int)\n",
        "        f1_thresh = f1_score(y_val, y_pred_thresh)\n",
        "        \n",
        "        if f1_thresh > best_f1_threshold:\n",
        "            best_f1_threshold = f1_thresh\n",
        "            best_threshold = threshold\n",
        "    \n",
        "    # Финальные предсказания с оптимальным порогом\n",
        "    y_pred_agg = (final_scores >= best_threshold).astype(int)\n",
        "    \n",
        "    # Метрики\n",
        "    accuracy = accuracy_score(y_val, y_pred_agg)\n",
        "    precision = precision_score(y_val, y_pred_agg, zero_division=0)\n",
        "    recall = recall_score(y_val, y_pred_agg)\n",
        "    f1 = f1_score(y_val, y_pred_agg)\n",
        "    \n",
        "    results_calibration.append({\n",
        "        'w_rules': w_rules,\n",
        "        'w_ml': w_ml,\n",
        "        'threshold': best_threshold,\n",
        "        'Accuracy': accuracy,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1-score': f1\n",
        "    })\n",
        "    \n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_w_rules = w_rules\n",
        "        best_w_ml = w_ml\n",
        "        best_final_scores = final_scores.copy()\n",
        "        best_y_pred_aggregated = y_pred_agg.copy()\n",
        "        best_threshold_final = best_threshold\n",
        "\n",
        "calibration_results_df = pd.DataFrame(results_calibration)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Результаты калибровки весов:\")\n",
        "print(\"=\" * 60)\n",
        "display(calibration_results_df.sort_values('F1-score', ascending=False))\n",
        "\n",
        "print(f\"\\n✓ Оптимальные веса:\")\n",
        "print(f\"  w_rules = {best_w_rules:.1f}\")\n",
        "print(f\"  w_ml = {best_w_ml:.1f}\")\n",
        "print(f\"  threshold = {best_threshold_final:.2f}\")\n",
        "print(f\"  F1-score (val) = {best_f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Визуализация результатов калибровки\n",
        "\n",
        "Визуализация результатов калибровки весов позволяет оценить влияние различных комбинаций весов на метрики качества и выбрать оптимальные значения.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Визуализация результатов калибровки\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# График F1-score в зависимости от w_rules\n",
        "axes[0].plot(calibration_results_df['w_rules'], calibration_results_df['F1-score'], \n",
        "             marker='o', linewidth=2, markersize=8)\n",
        "axes[0].axvline(x=best_w_rules, color='r', linestyle='--', label=f'Оптимум (w_rules={best_w_rules:.1f})')\n",
        "axes[0].set_xlabel('Вес правил (w_rules)')\n",
        "axes[0].set_ylabel('F1-score')\n",
        "axes[0].set_title('Зависимость F1-score от веса правил')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].legend()\n",
        "\n",
        "# Тепловая карта метрик\n",
        "metrics_pivot = calibration_results_df.pivot_table(\n",
        "    values=['F1-score', 'Precision', 'Recall'],\n",
        "    index='w_rules'\n",
        ")\n",
        "\n",
        "sns.heatmap(\n",
        "    metrics_pivot,\n",
        "    annot=True,\n",
        "    fmt='.3f',\n",
        "    cmap='YlOrRd',\n",
        "    ax=axes[1],\n",
        "    cbar_kws={'label': 'Значение метрики'}\n",
        ")\n",
        "axes[1].set_title('Метрики в зависимости от веса правил')\n",
        "axes[1].set_xlabel('Метрика')\n",
        "axes[1].set_ylabel('Вес правил (w_rules)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Оценка на тестовой выборке\n",
        "\n",
        "Финальная оценка выбранной модели с оптимальными весами агрегации на тестовой выборке. На данном этапе производится вычисление метрик качества, анализ матрицы ошибок и сравнение результатов модели только с ML-компонентом и модели с агрегацией ML и эвристических правил.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Вычисление risk_score для тестовой выборки\n",
        "print(\"Вычисление risk_score для тестовой выборки...\")\n",
        "\n",
        "if email_df is not None and 'email_content' in email_df.columns:\n",
        "    # Реальные вычисления\n",
        "    risk_scores_test = []\n",
        "    for idx in tqdm(range(len(X_test)), desc=\"  Обработка писем\", ncols=80):\n",
        "        if idx < len(email_df):\n",
        "            risk_score = compute_risk_score(email_df.iloc[idx]['email_content'])\n",
        "            risk_scores_test.append(risk_score)\n",
        "        else:\n",
        "            risk_scores_test.append(0)\n",
        "    risk_scores_test = np.array(risk_scores_test)\n",
        "else:\n",
        "    # Синтетические risk_score для демонстрации\n",
        "    np.random.seed(42)\n",
        "    risk_scores_test = np.random.randint(0, 100, size=len(y_test))\n",
        "    risk_scores_test[y_test == 1] = np.random.randint(30, 100, size=np.sum(y_test == 1))\n",
        "\n",
        "# Нормализация\n",
        "risk_scores_test_normalized = risk_scores_test / 100.0\n",
        "\n",
        "# Confidence scores от ML модели\n",
        "y_test_confidence = best_model.predict_proba(X_test)[:, 1] if hasattr(best_model, 'predict_proba') else None\n",
        "\n",
        "if y_test_confidence is None:\n",
        "    decision_scores = best_model.decision_function(X_test)\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    scaler = MinMaxScaler()\n",
        "    y_test_confidence = scaler.fit_transform(decision_scores.reshape(-1, 1)).ravel()\n",
        "\n",
        "# Агрегация с оптимальными весами\n",
        "final_scores_test = best_w_rules * risk_scores_test_normalized + best_w_ml * y_test_confidence\n",
        "\n",
        "# Предсказания с оптимальным порогом\n",
        "y_pred_test = (final_scores_test >= best_threshold_final).astype(int)\n",
        "\n",
        "# Метрики на тестовой выборке\n",
        "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
        "test_precision = precision_score(y_test, y_pred_test, zero_division=0)\n",
        "test_recall = recall_score(y_test, y_pred_test)\n",
        "test_f1 = f1_score(y_test, y_pred_test)\n",
        "test_roc_auc = roc_auc_score(y_test, final_scores_test)\n",
        "\n",
        "# Матрица ошибок\n",
        "test_cm = confusion_matrix(y_test, y_pred_test)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Результаты на тестовой выборке:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Accuracy:  {test_accuracy:.4f}\")\n",
        "print(f\"Precision: {test_precision:.4f}\")\n",
        "print(f\"Recall:   {test_recall:.4f}\")\n",
        "print(f\"F1-score:  {test_f1:.4f}\")\n",
        "print(f\"ROC-AUC:   {test_roc_auc:.4f}\")\n",
        "\n",
        "print(\"\\nМатрица ошибок:\")\n",
        "print(test_cm)\n",
        "\n",
        "# Анализ False Positive и False Negative\n",
        "tn, fp, fn, tp = test_cm.ravel()\n",
        "print(f\"\\nАнализ ошибок:\")\n",
        "print(f\"  True Negatives (TN):  {tn}\")\n",
        "print(f\"  False Positives (FP): {fp} ({fp/len(y_test)*100:.2f}%)\")\n",
        "print(f\"  False Negatives (FN): {fn} ({fn/len(y_test)*100:.2f}%)\")\n",
        "print(f\"  True Positives (TP):  {tp}\")\n",
        "\n",
        "test_metrics = {\n",
        "    'accuracy': float(test_accuracy),\n",
        "    'precision': float(test_precision),\n",
        "    'recall': float(test_recall),\n",
        "    'f1_score': float(test_f1),\n",
        "    'roc_auc': float(test_roc_auc),\n",
        "    'confusion_matrix': {\n",
        "        'tn': int(tn),\n",
        "        'fp': int(fp),\n",
        "        'fn': int(fn),\n",
        "        'tp': int(tp)\n",
        "    }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.1 Визуализация результатов на тестовой выборке\n",
        "\n",
        "Визуализация включает матрицу ошибок на тестовой выборке и сравнительный анализ метрик модели только с ML-компонентом и модели с агрегацией ML и эвристических правил.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Визуализация результатов\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Матрица ошибок\n",
        "sns.heatmap(\n",
        "    test_cm,\n",
        "    annot=True,\n",
        "    fmt='d',\n",
        "    cmap='Blues',\n",
        "    ax=axes[0],\n",
        "    cbar_kws={'label': 'Количество'}\n",
        ")\n",
        "axes[0].set_title(f'Матрица ошибок на тестовой выборке\\n({best_model_name} с агрегацией)')\n",
        "axes[0].set_xlabel('Предсказано')\n",
        "axes[0].set_ylabel('Реально')\n",
        "axes[0].set_xticklabels(['Legitimate', 'Phishing'])\n",
        "axes[0].set_yticklabels(['Legitimate', 'Phishing'])\n",
        "\n",
        "# Сравнение метрик: только ML vs ML + Rules\n",
        "metrics_comparison = {\n",
        "    'Метрика': ['Accuracy', 'Precision', 'Recall', 'F1-score'],\n",
        "    'Только ML': [\n",
        "        accuracy_score(y_test, best_model.predict(X_test)),\n",
        "        precision_score(y_test, best_model.predict(X_test), zero_division=0),\n",
        "        recall_score(y_test, best_model.predict(X_test)),\n",
        "        f1_score(y_test, best_model.predict(X_test))\n",
        "    ],\n",
        "    'ML + Rules': [test_accuracy, test_precision, test_recall, test_f1]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(metrics_comparison)\n",
        "comparison_melted = comparison_df.melt(\n",
        "    id_vars='Метрика',\n",
        "    value_vars=['Только ML', 'ML + Rules'],\n",
        "    var_name='Модель',\n",
        "    value_name='Значение'\n",
        ")\n",
        "\n",
        "sns.barplot(\n",
        "    data=comparison_melted,\n",
        "    x='Метрика',\n",
        "    y='Значение',\n",
        "    hue='Модель',\n",
        "    palette='Set2',\n",
        "    ax=axes[1]\n",
        ")\n",
        "axes[1].set_title('Сравнение метрик: только ML vs ML + Rules')\n",
        "axes[1].set_ylabel('Значение метрики')\n",
        "axes[1].set_ylim(0, 1)\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "axes[1].legend(title='Модель')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Сравнительная таблица метрик\n",
        "\n",
        "Сводная таблица метрик всех моделей на валидационной выборке и финальной модели с агрегацией на тестовой выборке. Таблица сохраняется в формате CSV для использования в ВКР.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Создание сравнительной таблицы для ВКР\n",
        "comparison_table = []\n",
        "\n",
        "# Добавляем результаты всех моделей на валидационной выборке\n",
        "for _, row in val_results_df.iterrows():\n",
        "    comparison_table.append({\n",
        "        'Модель': row['Модель'],\n",
        "        'Выборка': 'Validation',\n",
        "        'Accuracy': row['Accuracy'],\n",
        "        'Precision': row['Precision'],\n",
        "        'Recall': row['Recall'],\n",
        "        'F1-score': row['F1-score'],\n",
        "        'ROC-AUC': row['ROC-AUC']\n",
        "    })\n",
        "\n",
        "# Добавляем результаты лучшей модели с агрегацией на тестовой выборке\n",
        "comparison_table.append({\n",
        "    'Модель': f'{best_model_name} + Rules (aggregated)',\n",
        "    'Выборка': 'Test',\n",
        "    'Accuracy': test_accuracy,\n",
        "    'Precision': test_precision,\n",
        "    'Recall': test_recall,\n",
        "    'F1-score': test_f1,\n",
        "    'ROC-AUC': test_roc_auc\n",
        "})\n",
        "\n",
        "comparison_table_df = pd.DataFrame(comparison_table)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"Сравнительная таблица метрик (для ВКР):\")\n",
        "print(\"=\" * 80)\n",
        "display(comparison_table_df.round(4))\n",
        "\n",
        "# Сохранение таблицы в CSV\n",
        "comparison_table_df.to_csv(MODELS_DIR / 'comparison_metrics.csv', index=False, encoding='utf-8-sig')\n",
        "print(f\"\\n✓ Таблица сохранена: {MODELS_DIR / 'comparison_metrics.csv'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Сохранение лучшей модели\n",
        "print(\"Сохранение лучшей модели...\")\n",
        "with open(MODELS_DIR / 'best_model.pkl', 'wb') as f:\n",
        "    pickle.dump(best_model, f)\n",
        "print(f\"✓ Модель сохранена: {MODELS_DIR / 'best_model.pkl'}\")\n",
        "\n",
        "# Сохранение оптимальных весов агрегации\n",
        "aggregator_weights = {\n",
        "    'rules_weight': float(best_w_rules),\n",
        "    'ml_weight': float(best_w_ml),\n",
        "    'optimal_threshold': float(best_threshold_final),\n",
        "    'val_f1_score': float(best_f1),\n",
        "    'model_name': best_model_name\n",
        "}\n",
        "\n",
        "print(\"\\nСохранение весов агрегации...\")\n",
        "with open(MODELS_DIR / 'aggregator_weights.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(aggregator_weights, f, ensure_ascii=False, indent=2)\n",
        "print(f\"✓ Веса сохранены: {MODELS_DIR / 'aggregator_weights.json'}\")\n",
        "\n",
        "# Сохранение метрик на тестовой выборке\n",
        "print(\"\\nСохранение метрик на тестовой выборке...\")\n",
        "with open(MODELS_DIR / 'test_metrics.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(test_metrics, f, ensure_ascii=False, indent=2)\n",
        "print(f\"✓ Метрики сохранены: {MODELS_DIR / 'test_metrics.json'}\")\n",
        "\n",
        "print(\"\\n✓ Все результаты успешно сохранены!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
